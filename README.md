# Mastering Data-Wrangling & EDA

**Goal:** become fluent in **data-wrangling** and **exploratory data analysis (EDA)** through a sequence of replications and mini-projects.  
**Stack:** Python • SQL • JupyterLab

## Overview

This repo is my hands-on journey to master data cleaning, transformation, validation, and exploratory analysis. I start by **replicating published processes** to learn best practices, then build **small end-to-end projects** where I:

1) **Ingest** raw data (CSV/JSON/Excel/SQL/API)  
2) **Clean** (types, missing, duplicates, text/date parsing, de-noising)  
3) **Transform** (reshape, join, encode, validate) for the task at hand  
4) **Export** a tidy, documented dataset  
5) **EDA** (visuals, summaries, diagnostics, simple baselines)

Everything is reproducible and version-controlled.


## Learning Path

- **Phase 1 — Replications (Foundations)**
  - Recreate reputable cleaning & visualization workflows to absorb conventions.
  - **Kickoff:** Netflix dataset cleaning + analysis + visualization  
    Source: Kaggle — *“Netflix Data Cleaning, Analysis and Visualization”*  
    Link: https://www.kaggle.com/datasets/ariyoomotade/netflix-data-cleaning-analysis-and-visualization

- **Phase 2 — Mini-Projects (End-to-End)**
  - Pick a raw dataset, define a question, and ship a small, complete pipeline:
    - Raw → Clean → Transform → Export → EDA report (notebook/markdown)
    - Different data types each time (APIs, messy text, time series, etc.)
